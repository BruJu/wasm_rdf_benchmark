#!/usr/bin/env python3

import datetime
import os
import random
import subprocess
import sys

os.chdir(os.path.dirname(sys.argv[0]))

KINDS = {
    "fast"             : [ "rust", "rdfjsdataset"               ],
    "light"            : [ "rust", "rdfjsdataset"               ],
    "tree"             : [ "rust", "rdfjsdataset"               ],
    "tree_anti"        : [ "rust", "rdfjsdataset"               ],
    "full"             : [ "rust", "rdfjsdataset"               ],
    "array"            : [ "rust", "rdfjsdataset"               ],
    "fast_array"       : [ "rust", "rdfjsdataset"               ],
    "tree_array"       : [ "rust", "rdfjsdataset"               ],
    "full_array"       : [ "rust", "rdfjsdataset"               ],
    "wrap_tree"        : [         "rdfjsdataset"               ],
    "wrap_fast"        : [         "rdfjsdataset"               ],
    "wasm_tree"        : [         "rdfjsdataset", "rdfjsstore" ], # Identifier List, Shared Mapping
    "wasm_tree_FI"     : [         "rdfjsdataset"               ], # Always Forest  , Independant Mapping
    "wasm_tree_FS"     : [         "rdfjsdataset"               ], # Always Forest  , Shared Mapping
    "wasm_tree_II"     : [         "rdfjsdataset"               ], # Identifier List, Independant Mapping
    "graphy"           : [         "rdfjsdataset"               ],
    "n3"               : [                         "rdfjsstore" ]
}

TOOLS = {
    "rust"        : "./b_sophia/target/release/wasm_rdf_benchmark {task} '{filename}' {kind}",
    "rdfjsdataset": "./b_nodejs/benchDataset.js                   {task} '{filename}' {kind}",
    "rdfjsstore"  : "./b_nodejs/benchStore.js                     {task} '{filename}' {kind}"
}

FILES = [
    ("./data/persondata_en_10k.ttl",    10_000),
    ("./data/persondata_en_20k.ttl",    20_000),
    ("./data/persondata_en_40k.ttl",    40_000),
    ("./data/persondata_en_80k.ttl",    80_000),
    ("./data/persondata_en_100k.ttl",  100_000),
    ("./data/persondata_en_500k.ttl",  500_000),
    ("./data/persondata_en_1M.ttl",  1_000_000),
    # ("./data/persondata_en.ttl",    10_310_000),
]

HEADER = "t_load,m_graph,m_graph_end,t_first,t_rest,t_second,t_second_rest"

QUERIES = [ "S", "SG", "PO", "POG" ]

def main():
    if len(sys.argv) != 4:
        print(sys.argv)
        print("usage: {} <task> <tool> <kind>".format(sys.argv[0]), file=sys.stderr)
        print("  available tasks: {}"   .format(",".join(QUERIES)))
        print("  available tools: {}"   .format(",".join(TOOLS)  ))
        print("  available datasets: {}".format(",".join(KINDS)  ))
        exit(1)
        
    args = sys.argv[1:]
    task = args[0]
    tool = args[1]
    kind = args[2]

    nb_iter = int(os.getenv("N", "4"))

    benchmark(task, tool, kind, nb_iter)

def benchmark(task, tool, kind, nb_iter):
    if task not in QUERIES:
        print(task)
        print("available tasks: " + ", ".join(QUERIES), file=sys.stderr)
        exit(2)
    if tool not in TOOLS:
        print("------" + tool)
        print("available tools: " + ", ".join(TOOLS), file=sys.stderr)
        exit(3)
    if kind not in KINDS:
        print("Invalid kind: " + kind)
        print("available kinds: " + ", ".join(KINDS), file=sys.stderr)
        exit(8)

    list_of_tasks = []

    for filename, size in FILES:
        for _ in range(nb_iter):
            t = (task, tool, kind, filename, size)
            list_of_tasks.append(t)
    
    run_benchmarks(list_of_tasks)

def run_benchmarks(list_of_tasks):
    random.shuffle(list_of_tasks)
    csv = "csv/bench_{}.csv".format(
        str(datetime.datetime.now())[:19].replace(' ', '-').replace(':', '-'),
    )

    file_exists = os.path.isfile(csv)

    with open(csv, 'w+') as f:
        try:
            print("WRITING", csv, file=sys.stderr)
            if not file_exists:
                f.write("tool,dataset,pattern,size,{}\n".format(HEADER))
                f.flush()

            for task, tool, kind, filename, size in list_of_tasks:
                one_benchmark(f, task, tool, kind, filename, size)

        except KeyboardInterrupt:
            sys.stdout.write("\r") # clear the '^C' that appears

def one_benchmark(f, task, tool, kind, filename, size):
    cmd = TOOLS[tool]
    cmd = cmd.format(task=task, filename=filename, kind=kind)
    res = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
    f.write("{},{},{},{},{}".format(tool, kind, task, size, res.stdout.decode('utf8')))
    f.flush()
    print("DONE", task, tool, kind, filename, file=sys.stderr)

# =============================================================================

def mass_benchmark():
    NB_ITER = 2

    list_of_tasks = []
    for task in QUERIES:
        for kind in KINDS:
            for filename, size in FILES:
                for _ in range(NB_ITER):
                    if "rust" in KINDS[kind]:
                        t = (task, "rust", kind, filename, size)
                        list_of_tasks.append(t)
                    
                    if "rdfjsdataset" in KINDS[kind]:
                        t = (task, "rdfjsdataset", kind, filename, size)
                        list_of_tasks.append(t)

    run_benchmarks(list_of_tasks)


def exportation_dataset_impact(NB_ITER):
    list_of_tasks = []
    for task in QUERIES:
        for kind in ["fast", "fast_array", "wrap_fast"]:
            for filename, size in FILES:
                for _ in range(NB_ITER):
                    if "rust" in KINDS[kind]:
                        t = (task, "rust", kind, filename, size)
                        list_of_tasks.append(t)
                    
                    if "rdfjsdataset" in KINDS[kind]:
                        t = (task, "rdfjsdataset", kind, filename, size)
                        list_of_tasks.append(t)

    run_benchmarks(list_of_tasks)


def paperESWC(NB_ITER):
    list_of_tasks = []
    
    for task in ["S", "POG"]:

        for filename, size in FILES:
            for _ in range(NB_ITER):
                for kind in ["tree", "wasm_tree", "wrap_tree", "graphy"]:
                    if "rdfjsdataset" not in KINDS[kind]:
                        print("errrr")
                        exit()

                    t = (task, "rdfjsdataset", kind, filename, size)
                    list_of_tasks.append(t)
                
                for kind in ["wasm_tree", "n3"]:
                    if "rdfjsstore" not in KINDS[kind]:
                        print("errrr")
                        exit()
                    t = (task, "rdfjsstore", kind, filename, size)
                    list_of_tasks.append(t)

    run_benchmarks(list_of_tasks)


def paperESWC_barPlot(NB_ITER):
    list_of_tasks = []
    
    for task in ["S", "POG"]:
        for filename, size in [("./data/persondata_en_1M.ttl", 1_000_000)]:
            for _ in range(NB_ITER):
                for kind in ["tree", "wasm_tree", "wrap_tree", "graphy", "wasm_tree_FI", "wasm_tree_FS", "wasm_tree_II"]:
                    if "rdfjsdataset" in KINDS[kind]:
                        t = (task, "rdfjsdataset", kind, filename, size)
                        list_of_tasks.append(t)
    
    run_benchmarks(list_of_tasks)

def measure_javascript(NB_ITER):
    list_of_tasks = []
    for task in QUERIES:
        for kind in ["fast", "fast_array", "tree", "wasm_tree", "array", "graphy"]:
            for filename, size in FILES:
                for _ in range(NB_ITER):
                    if "rdfjsdataset" in KINDS[kind]:
                        t = (task, "rdfjsdataset", kind, filename, size)
                        list_of_tasks.append(t)

    run_benchmarks(list_of_tasks)

def measure_tree(NB_ITER):
    list_of_tasks = []
    for task in ["POG"]:
        for kind in ["tree", "tree_anti", "tree_array"]:
            for filename, size in FILES:
                for _ in range(NB_ITER):
                    if "rust" in KINDS[kind]:
                        t = (task, "rust", kind, filename, size)
                        list_of_tasks.append(t)

    run_benchmarks(list_of_tasks)


def store_or_dataset(NB_ITER):

    list_of_tasks = []
    
    task = "POG" # S
    
    for filename, size in FILES:
        for _ in range(NB_ITER):
            for kind in ["wasm_tree"]:
                t = (task, "rdfjsdataset", kind, filename, size)
                list_of_tasks.append(t)
                t = (task, "rdfjsstore", kind, filename, size)
                list_of_tasks.append(t)

    run_benchmarks(list_of_tasks)


def oooh():
    NB_ITER = 20
    list_of_tasks = []

    for _ in range(NB_ITER):
        for size in range(5000, 100000, 5000):
            t = ("POG", "rdfjsdataset", "wasm_tree", 'data/cpersondata_en_' + str(size) + ".ttl", size)
            list_of_tasks.append(t)

        for filename, size in FILES:
            for task in QUERIES:
                for kind in ["fast", "fast_array", "wrap_fast", "tree", "wasm_tree", "array", "graphy"]:
                    if "rdfjsdataset" in KINDS[kind]:
                        t = (task, "rdfjsdataset", kind, filename, size)
                        list_of_tasks.append(t)
            
                for kind in ["fast", "fast_array", "wrap_fast", "tree"]:
                    if "rust" in KINDS[kind]:
                        t = (task, "rust", kind, filename, size)
                        list_of_tasks.append(t)
    
    run_benchmarks(list_of_tasks)


def ooop():
    NB_ITER = 40
    list_of_tasks = []

    for task in QUERIES:
        for kind in ["wrap_tree", "tree", "wasm_tree", "graphy"]:
            for filename, size in FILES:
                if size != 1000000:
                    continue

                for _ in range(NB_ITER):
                    if "rdfjsdataset" in KINDS[kind]:
                        t = (task, "rdfjsdataset", kind, filename, size)
                        list_of_tasks.append(t)
    
    
    run_benchmarks(list_of_tasks)

paperESWC(50)
#paperESWC(50)

paperESWC_barPlot(20)

#store_or_dataset(10)
